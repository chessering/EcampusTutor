# pdf_lecture_transform.py
import os, re, io, json, time, base64, random
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import math # math ëª¨ë“ˆ ì„í¬íŠ¸
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from openai import RateLimitError, APIError

from PIL import Image as PILImage, ImageStat
from pdf2image import convert_from_path

import hashlib

# PDF ì¶œë ¥
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, KeepTogether, PageBreak, KeepInFrame
from reportlab.platypus import Image
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import mm
from reportlab.pdfbase.cidfonts import UnicodeCIDFont
from reportlab.pdfbase import pdfmetrics
from reportlab.lib import colors
from reportlab.lib.utils import ImageReader

# ======================== ì‚¬ìš©ì ì„¤ì • ========================
#PDF_FILE            = "./downloads/Computer Architecture_230427-Branch Prediction 2_-_230504_043850.pdf"      # ì…ë ¥ PDF
# WORKDIR              = "./khunote_pdf_run"
PDF_FILE = os.getenv("PDF_FILE")
WORKDIR = os.getenv("WORKDIR", "./output")
MODE = os.getenv("MODE", "quiz") 
MODEL_VISION         = "gpt-4o-mini"  # ì´ë¯¸ì§€ ì…ë ¥ ì§€ì› ëª¨ë¸ë¡œ í†µì¼
LANG                 = "ko"           # "ko" / "en"
# MODE                 = "quiz"      # "summary" | "blank" | "quiz"

if not PDF_FILE:
    raise ValueError("í™˜ê²½ë³€ìˆ˜ PDF_FILEì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

if not os.path.exists(PDF_FILE):
    raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_FILE}")

# POPPLER_PATH         = "../poppler/poppler-25.07.0/Library/bin"  # Windows ì˜ˆì‹œ. macOS/LinuxëŠ” None
POPPLER_PATH         = None
DPI                  = 150            # pdf -> image dpi
MAX_WIDTH            = 1280           # ì „ì†¡ ì „ ì´ë¯¸ì§€ ì¶•ì†Œ í­
JPEG_QUALITY         = 80             # ì „ì†¡ ì „ JPEG ì••ì¶• í’ˆì§ˆ
REQUEST_INTERVAL_SEC = 0.35           # ê°„ë‹¨ Throttle
MAX_RETRIES          = 6              # 429/5xx ì¬ì‹œë„ íšŸìˆ˜
BASE_BACKOFF         = 0.8            # ì§€ìˆ˜ ë°±ì˜¤í”„ ì‹œì‘ê°’(ì´ˆ)
MAX_IMAGES_PER_CALL  = 1              # í˜ì´ì§€ ë‹¨ìœ„ í˜¸ì¶œì´ë¯€ë¡œ 1 ì¶”ì²œ
PDF_INCLUDE_IMAGES   = True           # ìš”ì•½/ë¹ˆì¹¸ PDFì— í•˜ì´ë¼ì´íŠ¸ í˜ì´ì§€ ì¸ë„¤ì¼ í¬í•¨
SUMMARY_JSON_PATH   = os.path.join(WORKDIR, "KHUNote_summary.json")
BLANK_JSON_PATH     = os.path.join(WORKDIR, "KHUNote_blank.json")
ALWAYS_CLEAN_PAGES   = False   
MAX_AGGREGATE_PROMPT_BYTES = 200 * 1024 

# ===== JSON ì •í™•ì„± ê°•í™”ë¥¼ ìœ„í•œ ì„¤ì • =====
ENFORCE_JSON = True             # í†µí•© ìš”ì•½/ë¹ˆì¹¸/í€´ì¦ˆ ë‹¨ê³„ì—ì„œ JSONë§Œ ë°›ë„ë¡ ì••ë°•
JSON_AUTOFIX = True             # ê²½ë¯¸í•œ JSON ì˜¤ë¥˜(í™‘ë”°ì˜´í‘œ, íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ ë“±) ìë™ ë³µêµ¬ ì‹œë„

# ì²´í¬í¬ì¸íŠ¸ / ì‚°ì¶œë¬¼
os.makedirs(WORKDIR, exist_ok=True)
CHECKPOINT_PATH      = os.path.join(WORKDIR, "page_summaries_checkpoint.json")
HIGHLIGHT_JSON_PATH  = os.path.join(WORKDIR, "highlight_pages.json")
SUMMARY_PDF_PATH     = os.path.join(WORKDIR, "KHUNote_summary.pdf")
BLANK_PDF_PATH       = os.path.join(WORKDIR, "KHUNote_blank.pdf")
QUIZ_JSON_PATH       = os.path.join(WORKDIR, "KHUNote_quiz.json")
PAGE_SUMMARIES_JSON_PATH = os.path.join(WORKDIR, "KHUNote_page_summaries.json")

QUIZ_ALLOW_SHORT_ANSWER = True


# í•œê¸€ í°íŠ¸(ReportLab)
pdfmetrics.registerFont(UnicodeCIDFont('HYSMyeongJo-Medium'))

# ===== ì‹¤í–‰ ì‚°ì¶œë¬¼ ì´ë¦„ ì¶©ëŒ ë°©ì§€/ê´€ë¦¬ =====
def unique_path(path: str) -> str:
    """ì´ë¯¸ ìˆìœ¼ë©´ _1, _2 ... ì‹ìœ¼ë¡œ ìœ ë‹ˆí¬ ê²½ë¡œ ë°˜í™˜"""
    if not os.path.exists(path): 
        return path
    base, ext = os.path.splitext(path)
    i = 1
    while True:
        cand = f"{base}_{i}{ext}"
        if not os.path.exists(cand):
            return cand
        i += 1

RUN_TAG = time.strftime("%Y%m%d-%H%M%S")  # ì‹¤í–‰ ì‹œê° íƒœê·¸

def with_timestamp(path: str) -> str:
    """íŒŒì¼ëª…ì— ì‹¤í–‰ ì‹œê° íƒœê·¸ë¥¼ ìë™ ë¶€ì—¬"""
    base, ext = os.path.splitext(path)
    return f"{base}_{RUN_TAG}{ext}"

def read_text(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception:
        return ""

def write_text(path: str, s: str):
    with open(path, "w", encoding="utf-8") as f:
        f.write(s)

INPUT_HASH_PATH = os.path.join(WORKDIR, "input_pdf_hash.txt")

# OPENAI_API_KEY ì‚¬ìš©
load_dotenv()
client = OpenAI()

# ===== ì¶œë ¥ íŒŒì¼ëª… =====
SUMMARY_JSON_PATH = unique_path(with_timestamp(SUMMARY_JSON_PATH))
BLANK_JSON_PATH   = unique_path(with_timestamp(BLANK_JSON_PATH))
SUMMARY_PDF_PATH  = unique_path(with_timestamp(SUMMARY_PDF_PATH))
BLANK_PDF_PATH    = unique_path(with_timestamp(BLANK_PDF_PATH))
QUIZ_JSON_PATH    = unique_path(with_timestamp(QUIZ_JSON_PATH))

#fixedimage flowable
class FixedImage(Image):
    """
    LayoutErrorë¥¼ í”¼í•˜ê¸° ìœ„í•´ ReportLabì˜ Image Flowableì„ ìƒì†ë°›ì•„ wrap ë¡œì§ì„ ê³ ì •í•©ë‹ˆë‹¤.
    ReportLabì´ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ë¥¼ ì˜ëª» í•´ì„í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    def __init__(self, filename, width=None, height=None, **kw):
        # FixedImageë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” filenameì— ImageReader ê°ì²´ê°€ ì•„ë‹Œ íŒŒì¼ ê²½ë¡œ(str)ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
        
        # 1. ImageReaderë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë°ì´í„° ë¡œë“œ (ReportLab ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ë¨)
        # ì´ ì‹œì ì— ReportLabì´ íŒŒì¼ì—ì„œ í¬ê¸°ë¥¼ ì½ìœ¼ë ¤ ì‹œë„í•©ë‹ˆë‹¤.
        # ImageReader ê°ì²´ëŠ” ì´ë¯¸ì§€ê°€ ë¡œë“œëœ ìƒíƒœì˜ ê°ì²´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        try:
            self._img_data = ImageReader(filename)
            original_w = self._img_data.getSize()[0]
            original_h = self._img_data.getSize()[1]
        except Exception as e:
            # ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ê±°ë‚˜ ì˜ëª»ëœ ê²½ìš°, ì•ˆì „í•œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            self._img_data = None
            original_w = 1  # Safe defaults
            original_h = 1

        # 2. ì›í•˜ëŠ” drawWidth/drawHeightë¥¼ ê³„ì‚°
        if width is None and height is None:
            # í¬ê¸°ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ReportLabì´ ê¸°ë³¸ì ìœ¼ë¡œ 100% í”„ë ˆì„ ë„ˆë¹„ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ê³„í•  ìˆ˜ë„ ìˆì§€ë§Œ,
            # ì—¬ê¸°ì„œëŠ” ì•ˆì „ì„ ìœ„í•´ 1ì¸ì¹˜ í¬ê¸°ë¡œ ê¸°ë³¸ ì„¤ì •
            width = 1 * mm
            height = 1 * mm
        elif width is None:
            width = height / original_h * original_w
        elif height is None:
            height = width / original_w * original_h

        self.drawWidth = width
        self.drawHeight = height
        
        # 3. ë¶€ëª¨ Image Flowable ì´ˆê¸°í™”: FixedImageì˜ draw ë¡œì§ì´ drawWidth/drawHeightë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤.
        # ReportLab ImageëŠ” filename ì¸ìë¥¼ ë°›ì•„ ë‚´ë¶€ì ìœ¼ë¡œ ImageReaderë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        super().__init__(filename, width=self.drawWidth, height=self.drawHeight, **kw)
        
        # 4. wrap ë¡œì§ ê³ ì • (FixedImageì˜ í•µì‹¬)
        # self.drawWidthì™€ self.drawHeightê°€ ì´ë¯¸ ì„¤ì •ë˜ì—ˆìœ¼ë¯€ë¡œ, wrapì€ ì´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    def wrap(self, availWidth, availHeight):
        # ğŸš¨ ì—¬ê¸°ì„œ wrap ë©”ì„œë“œê°€ í•­ìƒ ë¯¸ë¦¬ ì„¤ì •ëœ ê³ ì • í¬ê¸°ë¥¼ ë°˜í™˜í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
        # ImageReader ì´ˆê¸°í™” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë”ë¼ë„, ì—¬ê¸°ì„œëŠ” ì˜¤ë¥˜ë¥¼ ë¬´ì‹œí•˜ê³ 
        # Table ë ˆì´ì•„ì›ƒì— í•„ìš”í•œ ì•ˆì „í•œ í¬ê¸°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        return self.drawWidth, self.drawHeight

    # wrapOnë„ wrapì„ í˜¸ì¶œí•˜ë„ë¡ ì„¤ì •í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    def wrapOn(self, *args, **kwargs):
        return self.wrap(args[1], kwargs.get('aH', args[-1]))

# ======================== SYSTEM PROMPT ë¡œë“œ ========================
def load_system_prompt(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

SYSTEM_PROMPT_PATH = Path("/app/scripts/system_prompts/visual_pdf_summary_prompt.txt")
SYSTEM_PROMPT_VISUAL = load_system_prompt(SYSTEM_PROMPT_PATH)

def sha256_str(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

SYSTEM_PROMPT_SIG = sha256_str(SYSTEM_PROMPT_VISUAL) 

def to_data_uri(image_path: str) -> str:
    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("ascii")
    ext = Path(image_path).suffix.lower().replace(".", "")
    mime = "jpeg" if ext in ("jpg", "jpeg") else "png"
    return f"data:image/{mime};base64,{b64}"

def _try_json_autofix(s: str):
    import re, json
    t = s.strip()
    # ì½”ë“œíœìŠ¤ ì œê±°
    t = re.sub(r"^```(json)?\s*|\s*```$", "", t, flags=re.S)
    # ë°”ê¹¥ { ... }ë§Œ ì¶”ì¶œ
    i, j = t.find("{"), t.rfind("}")
    if i == -1 or j == -1 or j <= i:
        return None
    t = t[i:j+1]
    # í™‘ë”°ì˜´í‘œ -> í°ë”°ì˜´í‘œ (í‚¤/ê°’ì— í•œì •)
    t = re.sub(r"\'([A-Za-z0-9_\-]+)\'\s*:", r'"\1":', t)
    t = re.sub(r':\s*\'([^\'\\]*)\'', r': "\1"', t)
    # íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ ì œê±°
    t = re.sub(r",\s*([\}\]])", r"\1", t)
    try:
        return json.loads(t)
    except:
        return None

# ======================== ìœ í‹¸(ë¡œê·¸/íŒŒì¼/ì²´í¬í¬ì¸íŠ¸) ========================
def log(msg: str): print(msg, flush=True)

def human_page(i: int, total: int) -> str:
    return f"p{i}/{total}"

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)
    return path

def load_checkpoint() -> Dict[str, str]:
    if os.path.exists(CHECKPOINT_PATH):
        with open(CHECKPOINT_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_checkpoint(cp: Dict[str, str]):
    with open(CHECKPOINT_PATH, "w", encoding="utf-8") as f:
        json.dump(cp, f, ensure_ascii=False, indent=2)
        
# ğŸš¨ [ì¶”ê°€ 1] ì´ë¯¸ì§€ ê²½ë¡œ ê´€ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° í´ë˜ìŠ¤
@dataclass
class ImagePaths:
    # 1-based page number to image file path
    page_to_path: Dict[int, str]

# ======================== OpenAI í˜¸ì¶œ ê³µí†µ ========================
def call_openai_with_retry(model: str, content_payload: list):
    """429/5xxì— ëŒ€í•´ ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„, ìš”ì²­ ê°„ ê°„ë‹¨ ëŒ€ê¸°"""
    attempt = 0
    while True:
        try:
            time.sleep(REQUEST_INTERVAL_SEC)
            resp = client.responses.create(
                model=model,
                input=[{"role":"user","content":content_payload}]
            )
            return resp
        except (RateLimitError, APIError) as e:
            attempt += 1
            if attempt > MAX_RETRIES: raise
            sleep_for = BASE_BACKOFF * (2 ** (attempt-1)) * (1 + random.random()*0.2)
            # 429ì¸ ê²½ìš° retry-afterë„ ì¡´ì¤‘
            if isinstance(e, RateLimitError):
                retry_after = getattr(e, "response", None) and e.response.headers.get("retry-after")
                if retry_after:
                    try: sleep_for = float(retry_after)
                    except: pass
            log(f"[WARN] transient error. retry {attempt}/{MAX_RETRIES} after {sleep_for:.2f}s")
            time.sleep(sleep_for)
        except TypeError as e:
            # âœ… êµ¬ë²„ì „ SDK / íŒŒë¼ë¯¸í„° ë¯¸ì§€ì› â†’ ì¦‰ì‹œ ìƒìœ„ í´ë°±ì´ ì‹œë„ë˜ë„ë¡ ì¬ë°œìƒ
            raise

# ======================== ì´ë¯¸ì§€ ì „ì²˜ë¦¬/í•´ì‹œ ========================
def shrink_and_encode_image(image_path: str, max_width: int = MAX_WIDTH, jpeg_quality: int = JPEG_QUALITY) -> str:
    """ì´ë¯¸ì§€ë¥¼ ê°€ë¡œ max_widthë¡œ ì¶•ì†Œ + JPEG ì••ì¶• í›„ data URI ë°˜í™˜"""
    im = PILImage.open(image_path).convert("RGB")
    w, h = im.size
    if w > max_width:
        new_h = int(h * (max_width / w))
        im = im.resize((max_width, new_h), PILImage.LANCZOS)
    buf = io.BytesIO()
    im.save(buf, format="JPEG", quality=jpeg_quality, optimize=True, progressive=True)
    b64 = base64.b64encode(buf.getvalue()).decode("ascii")
    return f"data:image/jpeg;base64,{b64}"

def sha256_file(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()

def average_hash(path: str, hash_size: int = 8) -> str:
    """
    ì™¸ë¶€ ì˜ì¡´ì„± ì—†ì´ êµ¬í˜„í•œ aHash (í¼ì…‰ì¶”ì–¼ ì¤‘ë³µ íƒì§€ìš©).
    """
    im = PILImage.open(path).convert("L").resize((hash_size, hash_size))
    pixels = list(im.getdata())
    avg = sum(pixels)/len(pixels)
    bits = "".join("1" if p > avg else "0" for p in pixels)
    # 4ë¹„íŠ¸ì”© â†’ hex
    return f"{int(bits, 2):0{hash_size*hash_size//4}x}"

def hamming(a: str, b: str) -> int:
    return bin(int(a, 16) ^ int(b, 16)).count("1")

def prompt_signature(prompt: str) -> str:
    return hashlib.sha256(prompt.encode("utf-8")).hexdigest()

def image_is_mostly_blank(path: str, entropy_threshold: float = 2.2) -> bool:
    """    
    ë¹ˆ/ì €ëŒ€ë¹„ ìŠ¬ë¼ì´ë“œ ê°„ì´ íƒì§€.
    ì•„ì£¼ ë‹¨ìˆœí•œ ë¹ˆ ìŠ¬ë¼ì´ë“œ/ì €ëŒ€ë¹„ íŒë‹¨(í…ìŠ¤íŠ¸ ê±°ì˜ ì—†ìŒ ì¶”ì •).
    - entropyê°€ ì§€ë‚˜ì¹˜ê²Œ ë‚®ìœ¼ë©´ ë¹ˆ í˜ì´ì§€ë¡œ ê°„ì£¼.
    """
    im = PILImage.open(path).convert("L")
    stat = ImageStat.Stat(im)
    # ë¶„ì‚° ê¸°ë°˜ ê°„ì´ entropy ì¶”ì •: log2(1+variance) ê·¼ì‚¬
    variance = stat.var[0]
    # import math # mathëŠ” íŒŒì¼ ìƒë‹¨ì— ì´ë¯¸ ì„í¬íŠ¸ë˜ì–´ ìˆìŒ
    entropy = math.log2(1.0 + variance)
    return entropy < entropy_threshold

# ======================== 1) PDF â†’ ì´ë¯¸ì§€ ========================
def pdf_to_images(pdf_path: str, out_dir: str, dpi: int = DPI, poppler_path: Optional[str] = POPPLER_PATH) -> List[str]:
    ensure_dir(out_dir)
    pages = convert_from_path(pdf_path, dpi=dpi, poppler_path=poppler_path)
    image_paths = []
    for i, im in enumerate(pages, start=1):
        p = os.path.join(out_dir, f"page_{i:04d}.jpg")
        im.convert("RGB").save(p, "JPEG", quality=92)
        image_paths.append(p)
    log(f"â–¶ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {len(image_paths)}p")
    return image_paths

# ======================== SYSTEM PROMPT í”„ë¦¬ì•°ë¸” ì£¼ì… ========================
def with_system_preamble(user_prompt: str) -> str:
    """
    ë¡œì§ ë³€ê²½ ì—†ì´ system í”„ë¡¬í”„íŠ¸ë¥¼ í…ìŠ¤íŠ¸ ì ‘ë‘ì‚¬ë¡œ ì„ìŒìŒ
    - responses.create(user ì—­í• ) ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - ëª¨ë¸ì—” í’ˆì§ˆ/ìŠ¤íƒ€ì¼/ì´ë¯¸ì§€ ì„ íƒ ê·œì¹™ì„ ì „ë‹¬í•˜ì§€ë§Œ,
      ì¶œë ¥ í˜•ì‹ì€ ê¸°ì¡´ ë§ˆí¬ë‹¤ìš´(+ ë§ˆì§€ë§‰ highlight JSON) ê´€ìŠµ ìœ ì§€
    """
    pre = SYSTEM_PROMPT_VISUAL.strip()
    return f"{pre}\n\n[USER TASK]\n{user_prompt}"


# ======================== 2) í˜ì´ì§€ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ========================
def page_summary_prompt(page_index: int, total_pages: int, lang: str = LANG) -> str:
    if lang == "ko":
        return (
            "ë‹¹ì‹ ì€ ëŒ€í•™ ê°•ì˜ ì¡°êµì…ë‹ˆë‹¤. ì•„ë˜ ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€(ìŠ¤ìº” í’ˆì§ˆ í¬í•¨)ë¥¼ ì½ê³ , í•µì‹¬ì„ ê³¼ë„í•œ ì¤‘ë³µ ì—†ì´ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.\n"
            "í•„ìš” ì‹œ ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ì´ì™¸ì˜ ì™¸ë¶€ ì‚¬ì´íŠ¸ë‚˜ ìë£Œë¥¼ ì°¸ì¡°í•˜ì—¬ ë‚´ìš©ì„ ë³´ê°•í•  ê²ƒ. ë‹¨, ì™¸ë¶€ ì‚¬ì´íŠ¸ë‚˜ ìë£ŒëŠ” ìš”ì•½ëœ ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì–´ì•¼ í•˜ê³ , ì‹ ë¢°ì„±ì´ ìˆì–´ì•¼ í•¨."
            "ìš”êµ¬ì‚¬í•­:\n"
            "1) ìˆ˜ì‹ì€ LaTeX ì¸ë¼ì¸ í‘œê¸°ë¡œ ìœ ì§€: $...$ (ì˜ˆ: $H(u,v)=\\frac{1}{1+D(u,v)}$)\n"
            "2) ê¸°ìˆ ì  ìš©ì–´Â·ê³ ìœ ëª…ì‚¬Â·ì•Œê³ ë¦¬ì¦˜Â·ì˜ë¬¸ í‘œê¸°ëŠ” ì˜ì–´ ê·¸ëŒ€ë¡œ ìœ ì§€(Fourier Transform, Laplacian, SVD ë“±)\n"
            "3) ê¸€ë¨¸ë¦¬í‘œëŠ” ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©: â€¢, â€“, â‘ , â‘¡ ë“± (í•˜ë‚˜ë§Œ ë°˜ë³µí•˜ì§€ ì•Šê¸°)\n"
            "4) ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì œê±°, í•µì‹¬ ì •ì˜/ê°€ì •/ì ˆì°¨/ì£¼ì˜ì ì„ ìš°ì„ \n"
            "5) í•´ë‹¹ í˜ì´ì§€ê°€ ì¤‘ìš”í•œ í˜ì´ì§€ì¸ì§€ is_critical(yes/no)ì™€ ì´ìœ ë¥¼ ë§ˆì§€ë§‰ì— í•œ ì¤„ë¡œ í‘œì‹œ\n"
            "í˜•ì‹ ì œì•½: ë°˜ë“œì‹œ 'ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸'ë§Œ ì¶œë ¥í•  ê²ƒ. ``` ì½”ë“œë¸”ë¡, JSON, YAML, HTMLì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ.\n"
            "ì¶œë ¥ í˜•ì‹(ë§ˆí¬ë‹¤ìš´):\n"
            f"### p{page_index}/{total_pages}\n"
            "â€¢ í•µì‹¬ ê°œë…:\n"
            "â€“ ì£¼ìš” ì •ì˜:\n"
            "â‘  ê³µì‹/ì¡°ê±´:\n"
            "â‘¡ ì ˆì°¨/ì•Œê³ ë¦¬ì¦˜:\n"
            "ì£¼ì˜/ì˜¤í•´ ì£¼ì˜:\n"
            "_is_critical: (yes/no, ì´ìœ  í•œ ì¤„)_\n"
        )
    else:
        return (
            "You are a university TA. Read the slide image and produce a concise summary with:\n"
            "1) Keep math in LaTeX $...$\n"
            "2) Preserve English technical terms as-is\n"
            "3) Vary bullets: â€¢, â€“, â‘ , â‘¡ ...\n"
            "4) Focus on definitions/assumptions/procedures/pitfalls\n"
            "5) End with _is_critical: (yes/no, one-line reason)_\n"
            f"### p{page_index}/{total_pages}\n"
        )

# ======================== 3) ë¹„ì „ í˜¸ì¶œ(í˜ì´ì§€ ìš”ì•½) ========================
def gpt_vision_on_image(prompt_text: str, image_path: str, model: str = MODEL_VISION) -> str:
    prompt_text = with_system_preamble(prompt_text) 
    data_uri = shrink_and_encode_image(image_path, max_width=MAX_WIDTH, jpeg_quality=JPEG_QUALITY)
    content = [
        {"type":"input_text","text":prompt_text},
        {"type":"input_image","image_url":data_uri}
    ]
    try:
        resp = call_openai_with_retry(model=model, content_payload=content)
        return (getattr(resp, "output_text", None) or "").strip()
    except TypeError:
        # ì´ë¯¸ì§€ ì…ë ¥ ë¯¸ì§€ì› í´ë°±(ì°¨ì„ ) â€” í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œë¼ë„ ìš”ì•½ ìƒì„±
        log("[WARN] TypeError in vision call. Falling back to text-only summary.")
        try:
            time.sleep(REQUEST_INTERVAL_SEC)
            chat = client.chat.completions.create(
                model=model,
                messages=[
                    {"role":"system","content":SYSTEM_PROMPT_VISUAL},
                    {"role":"user","content":f"{prompt_text}\n\n[ì°¸ê³ : ì´ë¯¸ì§€ ì…ë ¥ í´ë°± ê²½ë¡œ, í…ìŠ¤íŠ¸ ê¸°ì¤€ ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”.]"}
                ]
            )
            return (chat.choices[0].message.content or "").strip()
        except Exception as e:
            log(f"[ERROR] Fallback also failed: {e}")
            return ""

# ======================== 4) í˜ì´ì§€ ë‹¨ìœ„ ìš”ì•½ ========================
def summarize_pages(image_paths: List[str]) -> Dict[str, str]:
    start_time = 0
    cp = load_checkpoint()       # { "p1": "...", "p2": "...", ... }
    total = len(image_paths)
    updated = False
    
    for i, img in enumerate(image_paths, start=2):
        elapsed = time.time() - start_time  # start_timeì„ í•¨ìˆ˜ ì‹œì‘ë¶€ì— ì¶”ê°€
        log(f"â–¶ [{i}/{total}] ì§„í–‰ë¥ : {i/total*100:.1f}% | ê²½ê³¼: {elapsed:.0f}ì´ˆ")
        key = f"p{i}"
        # í˜„ì¬ ì´ë¯¸ì§€/í”„ë¡¬í”„íŠ¸ ì‹œê·¸ë‹ˆì²˜ ì¤€ë¹„
        img_sha = sha256_file(img)
        ah = average_hash(img)
        p_prompt = page_summary_prompt(i, total, LANG)
        p_sig = prompt_signature(p_prompt)
        
        # ê±´ë„ˆë›°ê¸° íŒì •
        cached = cp.get(key)
        skip = False
        if isinstance(cached, str):
            cp[key] = {"md": cached}
            cached = cp[key]
            save_checkpoint(cp)
            
        # âœ… ì•ˆì „í•œ ë¹„êµ (dictì¼ ë•Œë§Œ ë¹„êµ)
        if isinstance(cached, dict):
            if (cached.get("img_sha") == img_sha and
                cached.get("ahash") == ah and
                cached.get("prompt_sig") == p_sig and
                cached.get("sys_prompt_sig") == SYSTEM_PROMPT_SIG and   # âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í•´ì‹œë„ ë¹„êµ
                cached.get("model") == MODEL_VISION):
                skip = True

        if skip:
            log(f"â–¶ í˜ì´ì§€ ìš”ì•½ ê±´ë„ˆëœ€(ì²´í¬í¬ì¸íŠ¸ hit): {human_page(i,total)}")
            continue
        # ë¹„ìš© ì ˆê°: ë¹ˆ(ë˜ëŠ” ê±°ì˜ ë¹ˆ) ìŠ¬ë¼ì´ë“œ ê°ì§€ ì‹œ ì´ˆì†Œí˜• ìš”ì•½ë§Œ ìš”ì²­í•˜ê±°ë‚˜ ìŠ¤í‚µ ì˜µì…˜
        if image_is_mostly_blank(img):
            log(f"âš  ë¹ˆ/ì €ëŒ€ë¹„ ìŠ¬ë¼ì´ë“œ ê°ì§€: {human_page(i,total)} â†’ ê°„ë‹¨ ìš”ì•½ ì‹œë„")
            p_prompt += "\n\n[ì¶”ê°€ ê·œì¹™] ì´ í˜ì´ì§€ëŠ” ë¹ˆ/ì €ëŒ€ë¹„ ìŠ¬ë¼ì´ë“œë¡œ ê°ì§€ë¨. ì œëª©/ë©”íƒ€ë§Œ ê°„ë‹¨ ê¸°ë¡í•˜ê³  ìƒì„¸ëŠ” ìƒëµ."
        
        log(f"â–¶ í˜ì´ì§€ ìš”ì•½ ìš”ì²­: {human_page(i,total)}")
        out = gpt_vision_on_image(p_prompt, img, model=MODEL_VISION)
        out = sanitize_page_md(out)

        cp[key] = {
            "md": out,
            "img_sha": img_sha,
            "ahash": ah,
            "prompt_sig": p_sig,
            "sys_prompt_sig": SYSTEM_PROMPT_SIG,
            "model": MODEL_VISION
        }
        updated = True
        # ë§¤ í˜ì´ì§€ë§ˆë‹¤ ì €ì¥
        save_checkpoint(cp)
        
    if not updated:
        log("â„¹ ì²´í¬í¬ì¸íŠ¸ ì¼ì¹˜: ë³€ê²½ëœ í˜ì´ì§€ê°€ ì—†ì–´ ìƒˆ í˜¸ì¶œ ì—†ìŒ")
    return {k: v["md"] if isinstance(v, dict) and "md" in v else v for k, v in cp.items() if k.startswith("p")}

# ======================== 5) í†µí•© í”„ë¡¬í”„íŠ¸(ìš”ì•½/ë¹ˆì¹¸/í€´ì¦ˆ) ========================
AGG_PROMPT_BASE = (
    "ì•„ë˜ëŠ” ìŠ¬ë¼ì´ë“œ í˜ì´ì§€ë³„ ìš”ì•½ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì²­ëœ ì‚°ì¶œë¬¼ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.\n"
    "ê³µí†µ ê·œì¹™:\n"
    "- ìˆ˜ì‹ì€ LaTeX $...$ í˜•ì‹ ìœ ì§€\n"
    "- ê¸°ìˆ  ìš©ì–´/ê³ ìœ ëª…ì‚¬ëŠ” ì˜ì–´ ê·¸ëŒ€ë¡œ ìœ ì§€(Fourier, Laplacian, SVD ë“±)\n"
    "- ê¸€ë¨¸ë¦¬í‘œ ë‹¤ì–‘í™”(â€¢, â€“, â‘ , â‘¡ ...)\n"
    "- ë°˜ë³µ/êµ°ë”ë”ê¸° ì œê±°, ì‹œí—˜ ëŒ€ë¹„ í•µì‹¬ ìš°ì„ \n"
    "- (ìŠ¤íƒ€ì¼ ì˜ë„) ì„¹ì…˜ êµ¬ë¶„ì„ /í‘œ í—¤ë” ì—°í•œ í•˜ëŠ˜ìƒ‰(#EAF3FF)/í‘œ ì§€ë¸Œë¼, ì œëª© ì•„ë˜ í•œ ì¤„ ê³µë°±ì„ ì—¼ë‘ì— ë‘ê³  ì„œìˆ \n"
    "- (ì´ë¯¸ì§€ ì„ íƒ) í…ìŠ¤íŠ¸ ì´í•´ì— ì‹¤ì§ˆì  ë„ì›€ ë  ë•Œë§Œ ì´ë¯¸ì§€ë¥¼ ì–¸ê¸‰í•˜ê³ , ì¥ì‹/ì¤‘ë³µì€ ë°°ì œ\n"
    "- ì¤‘ìš” í˜ì´ì§€ë¥¼ íŒë‹¨í•´ highlight_pages(ì •ìˆ˜ ë°°ì—´)ë„ JSONìœ¼ë¡œ í•¨ê»˜ ì œì‹œ\n"
    "ì…ë ¥ì€ p1..pN í˜•íƒœì˜ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì´ë©°, ê° ë¸”ë¡ ëì—ëŠ” is_critical ë©”ëª¨ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
)

def prompt_aggregate_summary(all_pages_md: str) -> str:
    return (
        SYSTEM_PROMPT_VISUAL + "\n\n" + 
        AGG_PROMPT_BASE +
        "\n[ìš”ì²­]\n"
        "ì•„ë˜ í˜ì´ì§€ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ **ìš”ì•½ ë…¸íŠ¸(JSON)** í•œ ê°œ ê°ì²´ë¡œë§Œ ì¶œë ¥í•˜ë¼. ìŠ¤í‚¤ë§ˆëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:\n"
        "{\n"
        '  "meta": { "title": "string", "course": "string", "date": "string", "language": "ko" },\n'
        '  "style": { "section_rule": true, "heading_spacing_after": "1-line", "tables": {"header_color":"#EAF3FF","zebra_stripe":true}, "layout":{"avoid_manual_pagebreaks":true,"compact":true} },\n'
        '  "summaries": {\n'
        '    "standard": {\n'
        '        "sections": [ {\n' 
        '            "h2":"string",\n' 
        '            "paragraphs":["..."],\n' 
        '            "bullets":["..."],\n' 
        '            "tables":[{"title":"string","columns":["..."],"rows":[["..."]]}]\n' 
        '         } ],\n'
        '       "glossary:[["ìš©ì–´","ì„¤ëª…"]],\n'
        '       "checklist":["..."]\n'
        '   }\n'
        "  },\n"
        '  "highlight_pages": [] # ì‹¤ì œ ì¤‘ìš” í˜ì´ì§€ ë²ˆí˜¸\n'
        "}\n"
        "- ìµœì¢… ì‚°ì¶œë¬¼ì€ ë°˜ë“œì‹œ í•œêµ­ì–´(KO)ë¡œ ì‘ì„±í•˜ë¼.\n"
        "- ê³¼ë„í•œ ì¤‘ë³µ ì œê±°, ìˆ˜ì‹ì€ $...$ ìœ ì§€, ê¸°ìˆ  ìš©ì–´ ì˜ì–´ ìœ ì§€.\n"
        "\n[ì…ë ¥ ì›ë³¸(í˜ì´ì§€ ìš”ì•½)]\n" + all_pages_md
    )

def prompt_aggregate_blank(all_pages_md: str, min_clozes: int) -> str:
    return (
        SYSTEM_PROMPT_VISUAL + "\n\n" +
        AGG_PROMPT_BASE +
        "\n[ìš”ì²­]\n"
        f"**[ìµœìš°ì„  ëª©í‘œ]** ì•„ë˜ í˜ì´ì§€ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ì‹œí—˜ ëŒ€ë¹„ **í•µì‹¬ ìš©ì–´ ë¹ˆì¹¸ ì±„ìš°ê¸° ë¬¸ì œ(clozes)**ë¥¼ ìµœì†Œ {min_clozes}ê°œ ì´ìƒ **ë°˜ë“œì‹œ ìƒì„±**í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "ëŒ€í•™ ê°•ì˜ **ìš”ì•½ ë…¸íŠ¸**ë¥¼ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ì¶œë ¥í•˜ë¼(í•œ ê°œ JSON ê°ì²´). ìŠ¤í‚¤ë§ˆì˜ summaries í•„ë“œëŠ” clozes ë°°ì—´ì„ ì±„ìš°ê¸° ìœ„í•œ ë‚´ìš© ì°¸ì¡° ìš©ë„ë¡œ ì‚¬ìš©ë˜ë©°, clozes ë°°ì—´ ì±„ìš°ê¸°ê°€ ìµœìš°ì„ ì…ë‹ˆë‹¤:\n"
        "{\n"
        '  "meta": { "title": "string", "course": "string", "date": "string", "language": "ko" },\n'
        '  "style": { "section_rule": true, "heading_spacing_after": "1-line", "tables": {"header_color":"#EAF3FF","zebra_stripe":true}, "layout":{"avoid_manual_pagebreaks":true,"compact":true} },\n'
        '  "summaries": {\n'
        '    "short":    { "sections": [ { "h2":"string", "paragraphs":["..."], "bullets":["..."] } ], "glossary":[["ìš©ì–´","ì„¤ëª…"]], "checklist":["..."] },\n'
        '    "standard": { "sections": [ { "h2":"string", "paragraphs":["..."], "bullets":["..."] } ], "glossary":[["ìš©ì–´","ì„¤ëª…"]], "checklist":["..."] },\n'
        '    "detailed": { "sections": [ { "h2":"string", "paragraphs":["..."], "bullets":["..."] } ], "glossary":[["ìš©ì–´","ì„¤ëª…"]], "checklist":["..."] }\n'
        "  },\n"
        '  "clozes": [ { "text": "ë¬¸ì¥ ____ ë¡œ ê°€ë¦° ë¶€ë¶„", "answer": "ì •ë‹µ"} ],\n'
        "}\n"
        "\n[ì…ë ¥ ì›ë³¸(í˜ì´ì§€ ìš”ì•½)]\n" + all_pages_md
    )

def prompt_aggregate_quiz(
    all_pages_md: str,
    min_clozes: int,
    allow_short_answer: bool
) -> str:
    base = SYSTEM_PROMPT_VISUAL + "\n\n" + AGG_PROMPT_BASE + "\n[ìš”ì²­]\n"

    if allow_short_answer:
        # âœ… ê°ê´€ì‹ + ë‹¨ë‹µí˜• í˜¼í•© ëª¨ë“œ
        schema = (
            "**ì˜ˆìƒ ë¬¸ì œ ì„¸íŠ¸(JSON)** í•œ ê°œ ê°ì²´ë¡œë§Œ ì¶œë ¥í•˜ë¼:\n"
            "{\n"
            '  "meta": { "title": "string", "course": "string", "date": "string", "language": "ko" },\n'
            '  "multiple_choice": [\n'
            '    { "q": "ì§ˆë¬¸ ë¬¸ì¥", "options": ["ë³´ê¸°1","ë³´ê¸°2","ë³´ê¸°3","ë³´ê¸°4"], '
            '"answer_index": 1, "explanation": "ì„ íƒ/ìˆìœ¼ë©´ ê°„ë‹¨í•œ í•´ì„¤" }\n'
            "  ],\n"
            '  "short_answer": [\n'
            '    { "q": "ë‹¨ë‹µí˜• ì§ˆë¬¸ ë¬¸ì¥", '
            '"a": "ì •ë‹µ(í•œ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ êµ¬)", '
            '"answer_length": 5, '
            '"rubric": "ì±„ì  ê¸°ì¤€/í•µì‹¬ í‚¤ì›Œë“œ (ì„ íƒ)" }\n'
            "  ]\n"
            "}\n"
            f"- ìµœì†Œ {min_clozes}ë¬¸í•­ ì´ìƒ ì¶œì œí•˜ë˜, multiple_choiceì™€ short_answerë¥¼ ëª¨ë‘ í¬í•¨í•˜ê³  "
            "ë‘˜ ë‹¤ 1ë¬¸í•­ ì´ìƒì´ ë˜ë„ë¡ êµ¬ì„±í•˜ë¼.\n"
            "- short_answerëŠ” **ë°˜ë“œì‹œ 'ë‹¨ì–´' ë˜ëŠ” 'ì§§ì€ êµ¬(phrase)' ìˆ˜ì¤€ì˜ ë‹µ**ì´ ë‚˜ì˜¤ë„ë¡ í•  ê²ƒ.\n"
            "  - ì˜ˆ: ìš©ì–´ ì´ë¦„, ê¸°ë²• ì´ë¦„, êµ¬ì„± ìš”ì†Œ ì´ë¦„, 1ê°œì˜ ìˆ˜ì‹ ì´ë¦„ ë“±.\n"
            "  - **ì„¤ëª…í˜• ë¬¸ì¥(ì˜ˆ: '~ì„ ì˜ë¯¸í•œë‹¤', '~í•˜ëŠ” ê¸°ë²•ì´ë‹¤')ì„ aì— ì“°ì§€ ë§ ê²ƒ.**\n"
            "  - aëŠ” ê°€ëŠ¥í•˜ë©´ ëª…ì‚¬/ëª…ì‚¬êµ¬ í˜•íƒœë¡œë§Œ ì‘ì„±í•˜ë¼.\n"
            "- short_answerì˜ qëŠ”\n"
            "  - \"~ì„ ë¬´ì—‡ì´ë¼ê³  í•˜ë‚˜ìš”?\", \"~ë¥¼ ê°€ë¦¬í‚¤ëŠ” ìš©ì–´ëŠ”?\", \"~ì˜ ì´ë¦„ì€?\"ê³¼ ê°™ì´\n"
            "    **ì •ë‹µì´ ìš©ì–´ í•˜ë‚˜ë¡œ ë–¨ì–´ì§€ë„ë¡ ì§ˆë¬¸ì„ ì¬êµ¬ì„±**í•˜ë¼.\n"
            "- short_answerì˜ answer_lengthëŠ”\n"
            "  - aì— ë“¤ì–´ê°€ëŠ” ì‹¤ì œ ê¸€ì ìˆ˜(ê³µë°± ì œì™¸, í•œê¸€ ê¸°ì¤€)ë¥¼ ì •ìˆ˜ë¡œ ì ëŠ”ë‹¤.\n"
            "  - ì˜ˆ: aê°€ \"íŒŒì´í”„ë¼ì¸ í”ŒëŸ¬ì‹œ\"ë¼ë©´ answer_lengthëŠ” 8.\n"
        )
    else:
        # âœ… ê°ê´€ì‹ë§Œ ì¶œì œ ëª¨ë“œ
        schema = (
            "**ì˜ˆìƒ ë¬¸ì œ ì„¸íŠ¸(JSON)** í•œ ê°œ ê°ì²´ë¡œë§Œ ì¶œë ¥í•˜ë¼:\n"
            "{\n"
            '  "meta": { "title": "string", "course": "string", "date": "string", "language": "ko" },\n'
            '  "multiple_choice": [\n'
            '    { "q": "ì§ˆë¬¸ ë¬¸ì¥", "options": ["ë³´ê¸°1","ë³´ê¸°2","ë³´ê¸°3","ë³´ê¸°4"], '
            '"answer_index": 1, "explanation": "ì„ íƒ/ìˆìœ¼ë©´ ê°„ë‹¨í•œ í•´ì„¤" }\n'
            "  ]\n"
            "}\n"
            f"- ìµœì†Œ {min_clozes}ë¬¸í•­ ì´ìƒ ì¶œì œí•˜ê³ , "
            "**ë‹¨ë‹µí˜•(short_answer)ì€ ë§Œë“¤ì§€ ë§ ê²ƒ. ì˜¤ì§ multiple_choiceë§Œ ìƒì„±**í•˜ë¼.\n"
        )

    common_rules = (
        "- ê° ë¬¸í•­ì€ ê°•ì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³ , í˜ì´ì§€ ìš”ì•½ì—ì„œ ì¤‘ìš”í•˜ê²Œ í‘œì‹œëœ ê°œë…ì„ ìš°ì„ ì ìœ¼ë¡œ ë¬»ëŠ”ë‹¤.\n"
        "- multiple_choiceì˜ ë³´ê¸°(options)ëŠ” ì„œë¡œ ì¶©ë¶„íˆ í—·ê°ˆë¦´ ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•˜ë˜, ëª…ë°±íˆ í‹€ë¦° ì„ íƒì§€ëŠ” ë„£ì§€ ì•ŠëŠ”ë‹¤.\n"
        "- ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë¼.\n"
        "- ìµœì¢… ì‚°ì¶œë¬¼ì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ê³ , ë§ˆí¬ë‹¤ìš´/ì½”ë“œë¸”ë¡/ì„¤ëª… ë¬¸ì¥ì€ ë„£ì§€ ë§ ê²ƒ.\n"
        "\n[ì…ë ¥ ì›ë³¸(í˜ì´ì§€ ìš”ì•½)]\n"
    )

    return base + schema + common_rules + all_pages_md



# ======================== 6) í†µí•© LLM í˜¸ì¶œ(JSON ê°•ì œ) ========================
def call_llm_on_text(
    text_prompt: str,
    system_prompt: str = SYSTEM_PROMPT_VISUAL,
    *,
    max_output_tokens: int = 5000,
    temperature: float = 0.2,
    top_p: float = 0.9,
    min_sections: int = 6,
    min_glossary: int = 12,
    min_checklist: int = 10,
    retries: int = 1,
) -> dict:
    def _make_messages(extra_hint: str = ""):
        sys_content = [{"type": "input_text", "text": (system_prompt or "").strip()}]
        user_text = text_prompt if not extra_hint else (text_prompt + "\n\n" + extra_hint)

        user_content = [{"type": "input_text", "text": user_text}]
        # pdfëŠ” ì´ë¯¸ì§€ë„ ê°™ì´ ë³´ë‚¼ ê±°ë©´ ì—¬ê¸°ì„œ append
        # for p in page_images[:MAX_IMAGES_PER_CALL]:
        #     user_content.append({"type":"input_image","image_url":...})

        return [
            {"role": "system", "content": sys_content},
            {"role": "user", "content": user_content},
        ]

    def _extract_text(resp) -> str:
        out = getattr(resp, "output_text", None)
        if out is not None:
            return out
        try:
            return resp.output[0].content[0].text
        except Exception:
            return ""

    def _is_poor(obj: dict) -> bool:
        try:
            std = (obj.get("summaries") or {}).get("standard") or {}
            sections = std.get("sections") or []
            glossary = std.get("glossary") or []
            checklist = std.get("checklist") or []
            if len(sections) < min_sections: return True
            if len(glossary) < min_glossary: return True
            if len(checklist) < min_checklist: return True
            return False
        except Exception:
            return True

    # 1ì°¨ í˜¸ì¶œ
    messages = _make_messages()
    resp = client.responses.create(
        model=MODEL_VISION,
        input=messages,
        max_output_tokens=max_output_tokens,
        temperature=temperature,
        top_p=top_p,
    )
    raw_text = _extract_text(resp)

    try:
        obj = json.loads(raw_text)
    except Exception:
        fixed = _try_json_autofix(raw_text)
        obj = fixed if fixed is not None else {"raw": raw_text}

    # ë¶„ëŸ‰ ë¶€ì¡±í•˜ë©´ 1íšŒ ë³´ê°•
    if retries > 0 and isinstance(obj, dict) and _is_poor(obj):
        booster = (
            "âš ï¸ ë¶„ëŸ‰ ë³´ê°•:\n"
            f"- ì„¹ì…˜ ìµœì†Œ {min_sections}ê°œ(ê° ì„¹ì…˜ ë¬¸ë‹¨â‰¥3, ë¶ˆë¦¿â‰¥5)\n"
            f"- glossary ìµœì†Œ {min_glossary}ê°œ, checklist ìµœì†Œ {min_checklist}ê°œ\n"
            "- í‘œ ìµœì†Œ 2ê°œ(ë¹„êµ/ì ˆì°¨/ì¥ë‹¨ì ), ìˆ˜ì‹ $...$ ìœ ì§€\n"
            "- ì™¸ë¶€ ì§€ì‹ìœ¼ë¡œ ì •ì˜/ì˜ˆì‹œ/ì‘ìš© ììœ  ë³´ê°•(ì£¼ì œì™€ ì§ì ‘ ê´€ë ¨)\n"
            "- ë°˜ë“œì‹œ ìœ íš¨ JSONë§Œ ì¶œë ¥(ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ ì‹œ ìì²´ ë³µêµ¬)"
        )
        messages = _make_messages(extra_hint=booster)
        resp = client.responses.create(
            model=MODEL_VISION,
            input=messages,
            max_output_tokens=max_output_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        raw_text2 = _extract_text(resp)
        try:
            obj2 = json.loads(raw_text2)
        except Exception:
            fixed2 = _try_json_autofix(raw_text2)
            obj2 = fixed2 if fixed2 is not None else {"raw": raw_text2}

        if not isinstance(obj, dict) or _is_poor(obj):
            obj = obj2

    return obj

# ======================== 7) íŒŒì‹±/ë³´ì • ìœ í‹¸(JSON) ========================
def _strip_code_fences(s: str) -> str:
    # ```...``` ë¸”ë¡ì„ ì „ë¶€ ì œê±°
    return re.sub(r"```(?:[\s\S]*?)```", "", s).strip()

def _json_block_to_md(s: str) -> Optional[str]:
    """
    ```json ... ``` ë˜ëŠ” ë§¨ë°”ë‹¥ JSONì´ ë“¤ì–´ì˜¤ë©´ sections[].h2/paragraphs/bulletsë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜.
    """
    # ì½”ë“œíœìŠ¤ ë‚´ë¶€ JSON ì¶”ì¶œ
    m = re.search(r"```json\s+([\s\S]*?)\s+```", s, flags=re.I)
    raw = None
    if m:
        raw = m.group(1)
    else:
        # ë§¨ë°”ë‹¥ JSONì¼ ìˆ˜ë„ ìˆìŒ
        js = extract_json_object(s)
        if js is not None:
            raw = json.dumps(js, ensure_ascii=False)

    if not raw:
        return None

    try:
        obj = json.loads(raw)
    except Exception:
        # ê°„ë‹¨ ë³µêµ¬ ì‹œë„
        obj = _json_autofix(raw)

    if not isinstance(obj, dict):
        return None

    # í”í•œ êµ¬ì¡° ê°€ì •: {sections:[{h2, paragraphs, bullets}]}
    sections = []
    # 1) straight
    cand = obj.get("sections")
    # 2) KHUNote ìŠ¤íƒ€ì¼
    if cand is None:
        cand = obj.get("summaries", {}).get("standard", {}).get("sections")

    if isinstance(cand, list):
        for i, sec in enumerate(cand, start=1):
            h2 = (sec.get("h2") or f"Section {i}").strip()
            pars = [p for p in (sec.get("paragraphs") or []) if isinstance(p, str)]
            bulls = [b for b in (sec.get("bullets") or []) if isinstance(b, str)]
            buf = [f"### {h2}"]
            for p in pars:
                buf.append(p)
            for b in bulls:
                buf.append(f"â€¢ {b}")
            sections.append("\n".join(buf).strip())

    if not sections:
        return None
    return "\n\n".join(sections).strip()

def save_page_summaries_json(
    page_summaries: Dict[str, str],
    image_paths: List[str],
    checkpoint_path: str,
    out_path: str
) -> None:
    """
    summarize_pages()ê°€ ë°˜í™˜í•œ page_summariesì™€ checkpoint ë©”íƒ€ë¥¼ ë¬¶ì–´
    í˜ì´ì§€ ë‹¨ìœ„ JSONìœ¼ë¡œ ì €ì¥.
    """
    # ì²´í¬í¬ì¸íŠ¸(í•´ì‹œ, í”„ë¡¬í”„íŠ¸ ì‹œê·¸ë‹ˆì²˜ ë“±)ë¥¼ í•©ì³ì„œ í’ë¶€í•œ ë©”íƒ€ë¥¼ ë§Œë“ ë‹¤
    cp = {}
    try:
        if os.path.exists(checkpoint_path):
            with open(checkpoint_path, "r", encoding="utf-8") as f:
                raw = json.load(f)
                # ì´ì „ ë²„ì „ í˜¸í™˜: ë¬¸ìì—´ë§Œ ì €ì¥ëœ ê²½ìš° md í‚¤ë¡œ ê°ì‹¼ë‹¤
                for k, v in raw.items():
                    if isinstance(v, str):
                        cp[k] = {"md": v}
                    else:
                        cp[k] = v or {}
    except Exception as e:
        log(f"[WARN] checkpoint ì½ê¸° ì‹¤íŒ¨: {e}")

    total = len(image_paths)
    records = []
    for idx in range(1, total + 1):
        key = f"p{idx}"
        md = page_summaries.get(key, "")
        meta = cp.get(key, {})
        rec = {
            "page": idx,
            "summary_md": md,
            "image_path": image_paths[idx - 1] if 0 <= idx - 1 < len(image_paths) else None,
            # ì²´í¬í¬ì¸íŠ¸ì— ìˆìœ¼ë©´ ë©”íƒ€ë„ í•¨ê»˜ ì €ì¥ (ì—†ìœ¼ë©´ None)
            "img_sha": meta.get("img_sha"),
            "ahash": meta.get("ahash"),
            "prompt_sig": meta.get("prompt_sig"),
            "sys_prompt_sig": meta.get("sys_prompt_sig"),
            "model": meta.get("model"),
        }
        records.append(rec)

    payload = {
        "meta": {
            "source_pdf": os.path.abspath(PDF_FILE),
            "total_pages": total,
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "language": LANG,
            "model": MODEL_VISION,
        },
        "pages": records,
    }

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    log(f"âœ… í˜ì´ì§€ë³„ ìš”ì•½ JSON ì €ì¥: {out_path}")

def sanitize_page_md(s: str) -> str:
    """
    1) ```json ...```ì´ ìˆìœ¼ë©´ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    2) ê·¸ ì™¸ì˜ ì½”ë“œíœìŠ¤ëŠ” ì œê±°
    """
    md_from_json = _json_block_to_md(s)
    if md_from_json:
        return md_from_json
    return _strip_code_fences(s)

def _pages_for_section(sec: dict, highlight_pool: list) -> list:
    # 1) ì„¹ì…˜ì´ ì§ì ‘ pagesë¥¼ ê°–ê³  ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©
    if isinstance(sec.get("pages"), list) and sec["pages"]:
        return [p for p in sec["pages"] if isinstance(p, int)]
    # 2) images ì•ˆì— page ì§€ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    imgs = sec.get("images") or []
    for it in imgs:
        if isinstance(it, dict) and isinstance(it.get("page"), int):
            return [it["page"]]
    # 3) ë§ˆì§€ë§‰ ìˆ˜ë‹¨: highlight_pages í’€ì—ì„œ í•˜ë‚˜ì”© êº¼ë‚´ì„œ ë§¤ì¹­
    if highlight_pool:
        return [highlight_pool.pop(0)]
    return []

def parse_highlight_pages(llm_output: str, total_pages: int) -> List[int]:
    """
    LLM ì¶œë ¥ ë§ˆì§€ë§‰/ë³„ë„ ì¤„ì˜ {"highlight_pages":[...]} JSONì„ ì°¾ì•„ í˜ì´ì§€ ë°°ì—´ì„ ë°˜í™˜.
    ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ë²ˆí˜¸ëŠ” ì œê±°.
    """
    matches = re.findall(r'\{\s*"highlight_pages"\s*:\s*\[(.*?)\]\s*\}', llm_output, flags=re.S)
    if not matches: return []
    nums = re.findall(r'\d+', matches[-1])  # ë§ˆì§€ë§‰ ë§¤ì¹˜ ì‚¬ìš©
    pages = sorted({int(n) for n in nums if 1 <= int(n) <= total_pages})
    with open(HIGHLIGHT_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump({"highlight_pages": pages}, f, ensure_ascii=False, indent=2)
    return pages


def extract_json_object(text: str) -> Optional[dict]:
    """
    ëª¨ë¸ì´ ì•ë’¤ë¡œ ì¡ë‹´ì„ ë¶™ì´ì§€ ì•Šë„ë¡ ìœ ë„í–ˆì§€ë§Œ,
    í˜¹ì‹œ ëª°ë¼ì„œ ì²« ë²ˆì§¸ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•´ íŒŒì‹±í•œë‹¤.
    """
    # ê°€ì¥ ì•ì˜ '{'ë¶€í„° ë '}'ê¹Œì§€ ê·¼ì‚¬ ì¶”ì¶œ
    start = text.find('{')
    end = text.rfind('}')
    if start == -1 or end == -1 or end <= start:
        return None
    candidate = text[start:end+1]
    try:
        return json.loads(candidate)
    except Exception:
        # ë°±ì—…: í°ë”°ì˜´í‘œ ëˆ„ë½/ë¬¸ë²•ì˜¤ë¥˜ ë“±ì€ ì›ë¬¸ ì €ì¥ ìª½ìœ¼ë¡œ fallback
        return None

def _json_autofix(s: str) -> Optional[dict]:
    """
    ì‘ì€ë”°ì˜´í‘œâ†’í°ë”°ì˜´í‘œ, íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ ì œê±° ë“± ê²½ë¯¸í•œ ì˜¤ë¥˜ ìë™ ë³µêµ¬ ì‹œë„ë„
    """
    t = s.strip()
    # 1) ì½”ë“œë¸”ëŸ­/ë§ˆí¬ë‹¤ìš´ ì œê±°
    if t.startswith("```"):
        t = re.sub(r"^```(json)?\s*|\s*```$", "", t, flags=re.S)

    # 2) ê°€ì¥ ë°”ê¹¥ JSONë§Œ ì¶”ì¶œ
    start, end = t.find("{"), t.rfind("}")
    if start == -1 or end == -1 or end <= start:
        return None
    t = t[start:end+1]

    # 3) í™‘ë”°ì˜´í‘œë¥¼ í°ë”°ì˜´í‘œë¡œ(í‚¤/ë¬¸ìì—´ì— í•œì •) â€” ì•ˆì „í•˜ì§€ ì•Šì•„ ìµœì†Œ ì¹˜í™˜
    #   í‚¤: 'key': â†’ "key":
    t = re.sub(r"\'([A-Za-z0-9_\-]+)\'\s*:", r'"\1":', t)
    #   ê°’ ë¬¸ìì—´: : 'text' â†’ : "text"
    t = re.sub(r':\s*\'([^\'\\]*)\'', r': "\1"', t)

    # 4) íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ ì œê±° â†’ }, ], ,} ,]
    t = re.sub(r",\s*([\}\]])", r"\1", t)

    try:
        return json.loads(t)
    except Exception:
        return None

def _nonempty_tier(t: dict) -> bool:
    """
    ìš”ì•½ í‹°ì–´ ê°ì²´ì— ìœ íš¨í•œ ì„¹ì…˜ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    """
    return bool(t) and isinstance(t.get("sections", []), list) and len(t["sections"]) > 0

# ìŠ¤í‚¤ë§ˆ ê¸°ë³¸ê°’ ë³´ì •ê¸°
def _fill_summary_defaults(obj: dict, lang: str = "ko") -> dict:
    if "meta" not in obj: obj["meta"] = {}
    obj["meta"].setdefault("title", "Lecture Summary")
    obj["meta"].setdefault("course", "")
    obj["meta"].setdefault("date", time.strftime("%Y-%m-%d"))
    obj["meta"].setdefault("language", lang)

    if "style" not in obj: obj["style"] = {}
    style = obj["style"]
    style.setdefault("section_rule", True)
    style.setdefault("heading_spacing_after", "1-line")
    style.setdefault("tables", {"header_color":"#EAF3FF","zebra_stripe":True})
    style.setdefault("layout", {"avoid_manual_pagebreaks":True,"compact":True})

    if "summaries" not in obj: obj["summaries"] = {}
    for tier in ("short","standard","detailed"):
        obj["summaries"].setdefault(tier, {})
        t = obj["summaries"][tier]
        t.setdefault("sections", [])
        t.setdefault("glossary", [])
        t.setdefault("checklist", [])
        # ì„¹ì…˜ ì•ˆì „ í•„í„°
        fixed_sections = []
        for sec in t["sections"]:
            if not isinstance(sec, dict): continue
            sec.setdefault("h2","")
            sec.setdefault("paragraphs", [])
            sec.setdefault("bullets", [])
            sec.setdefault("tables", [])
            sec.setdefault("images", [])
            fixed_sections.append(sec)
        t["sections"] = fixed_sections

    obj.setdefault("references", [])
    obj.setdefault("highlight_pages", [])
    
    standard_sections = obj["summaries"]["standard"].get("sections")
    short_sections = obj["summaries"]["short"].get("sections")
    detailed_sections = obj["summaries"]["detailed"].get("sections")

    # í‘œì¤€ í‹°ì–´(standard)ê°€ ë¹„ì–´ ìˆì„ ê²½ìš°, shortë‚˜ detailedì—ì„œ ë‚´ìš©ì„ ë³µì‚¬í•´ ê¸°ë³¸ ë³¸ë¬¸ì„ ë³´ì¥
    if not standard_sections:
        if short_sections:
            obj["summaries"]["standard"]["sections"] = short_sections
        elif detailed_sections:
            obj["summaries"]["standard"]["sections"] = detailed_sections
    
    return obj

# ======================== 8) PDF ë Œë”ëŸ¬ (JSON/Markdown) ========================
def export_pdf_from_json(data: dict, outpath: str, image_paths_map: Optional[ImagePaths] = None): # ğŸš¨ [ìˆ˜ì • 2] image_paths_map ì¸ì ì¶”ê°€

    doc = SimpleDocTemplate(
        outpath, pagesize=A4,
        leftMargin=16*mm, rightMargin=16*mm,
        topMargin=16*mm, bottomMargin=16*mm
    )
    styles = getSampleStyleSheet()
    # í•œê¸€ ë³¸ë¬¸ ìŠ¤íƒ€ì¼
    styles.add(ParagraphStyle(name="BodyKR", parent=styles["Normal"], fontName="HYSMyeongJo-Medium", fontSize=10, leading=14))
    styles.add(ParagraphStyle(name="H2KR", parent=styles["Heading2"], fontName="HYSMyeongJo-Medium", spaceAfter=6))
    styles.add(ParagraphStyle(name="SmallGrey", parent=styles["Normal"], fontName="HYSMyeongJo-Medium", fontSize=8, textColor=colors.grey))
    # ğŸš¨ [ì¶”ê°€ 3] í•˜ì´ë¼ì´íŠ¸ ì„¹ì…˜ ìŠ¤íƒ€ì¼
    styles.add(ParagraphStyle(name="HighlightHeading", parent=styles["Normal"], fontName="HYSMyeongJo-Medium", fontSize=10, textColor=colors.HexColor("#0070C0"), spaceAfter=1*mm))
    styles.add(ParagraphStyle(name="HighlightPage", parent=styles["Normal"], fontName="HYSMyeongJo-Medium", fontSize=8, textColor=colors.grey, spaceBefore=0, spaceAfter=2*mm))

    story = []

    meta = data.get("meta", {})
    title = meta.get("title", "Lecture Summary")
    course = meta.get("course", "")
    date = meta.get("date", "")

    story.append(Paragraph(title, styles["Heading1"]))
    sub = " Â· ".join([x for x in [course, date] if x])
    if sub:
        story.append(Paragraph(sub, styles["SmallGrey"]))
    story.append(Spacer(1, 6*mm))

    style_cfg = data.get("style", {})
    rule_on = style_cfg.get("section_rule", True)
    table_cfg = data.get("style", {}).get("tables", {"header_color":"#EAF3FF","zebra_stripe":True}) # í…Œì´ë¸” ì„¤ì • ë‹¤ì‹œ ì½ê¸°
    
    # ğŸš¨ [ì¶”ê°€ 4] ì¸ë„¤ì¼ ì„¤ì •ì„ PDFì— ë°˜ì˜
    include_images = PDF_INCLUDE_IMAGES and image_paths_map and image_paths_map.page_to_path
    temp_files_to_clean = []
    def section_rule():
        # ì–‡ì€ êµ¬ë¶„ì„ 
        tbl = Table([[""]], colWidths=[doc.width])
        tbl.setStyle(TableStyle([
            ("LINEBELOW", (0,0), (-1,-1), 0.5, colors.HexColor("#DDDDDD")),
            ("LEFTPADDING",(0,0),(-1,-1),0), ("RIGHTPADDING",(0,0),(-1,-1),0),
            ("TOPPADDING",(0,0),(-1,-1),2), ("BOTTOMPADDING",(0,0),(-1,-1),6),
        ]))
        return tbl

    def bullets_flow(items: list):
        # ê°„ë‹¨ ë¶ˆë¦¿ ë Œë”(ë‘ ë ˆë²¨ê¹Œì§€ë§Œ)
        flows = []
        for it in items:
            flows.append(Paragraph(f"â€¢ {it}", styles["BodyKR"]))
        return flows

    def table_from_spec(spec: dict):
        title = spec.get("title")
        cols = spec.get("columns", [])
        rows = spec.get("rows", [])
        data_tbl = [cols] + rows
        t = Table(data_tbl, hAlign="LEFT")
        header_color = colors.HexColor(table_cfg.get("header_color", "#EAF3FF"))
        ts = [
            ("BACKGROUND", (0,0), (-1,0), header_color),
            ("TEXTCOLOR", (0,0), (-1,0), colors.black),
            ("FONTNAME", (0,0), (-1,-1), "HYSMyeongJo-Medium"),
            ("ALIGN", (0,0), (-1,0), "CENTER"),
            ("VALIGN", (0,0), (-1,-1), "TOP"),
            ("LEFTPADDING", (0,0), (-1,-1), 6),
            ("RIGHTPADDING", (0,0), (-1,-1), 6),
            ("TOPPADDING", (0,0), (-1,-1), 4),
            ("BOTTOMPADDING", (0,0), (-1,-1), 6),
            ("GRID", (0,0), (-1,-1), 0.25, colors.HexColor("#CCCCCC")),
        ]
        if table_cfg.get("zebra_stripe", True):
            for r in range(1, len(data_tbl)):
                if r % 2 == 1:
                    ts.append(("BACKGROUND", (0,r), (-1,r), colors.whitesmoke))
        t.setStyle(TableStyle(ts))
        flows = []
        if title:
            flows.append(Paragraph(title, styles["SmallGrey"]))
        flows.append(t)
        return flows
    
        # --- ì„¹ì…˜ì— ë§¤ì¹­í•  í˜ì´ì§€ ë²ˆí˜¸ ì„ íƒ ---
    def _pages_for_section(sec: dict, highlight_pool: list) -> list:
        # 1) ì„¹ì…˜ì— pagesê°€ ëª…ì‹œë¼ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if isinstance(sec.get("pages"), list) and sec["pages"]:
            return [p for p in sec["pages"] if isinstance(p, int)]
        # 2) imagesì— pageê°€ ëª…ì‹œë¼ ìˆìœ¼ë©´ ê·¸ ì¤‘ ì²« ë²ˆì§¸ ì‚¬ìš©
        imgs = sec.get("images") or []
        for it in imgs:
            if isinstance(it, dict) and isinstance(it.get("page"), int):
                return [it["page"]]
        # 3) ì•„ë¬´ íŒíŠ¸ë„ ì—†ìœ¼ë©´ highlight_pages í’€ì—ì„œ í•˜ë‚˜ì”© ì†Œë¹„
        if highlight_pool:
            return [highlight_pool.pop(0)]
        return []
    
        # --- ì„¹ì…˜ ë°”ë¡œ ë’¤ì— ì¸ë„¤ì¼ Flowable ì‚½ì… (í…Œì´ë¸” X) ---
    def _append_section_thumbnail(story, page_nums, image_paths_map, styles,
                                  max_w_mm=70, max_h_mm=60):
        if not page_nums or not image_paths_map or not image_paths_map.page_to_path:
            return []
        temp_files = []
        for p in page_nums:
            img_path = image_paths_map.page_to_path.get(p)
            if not img_path or not os.path.exists(img_path):
                continue
            try:
                with PILImage.open(img_path) as pil:
                    w, h = pil.size or (1, 1)
                    aspect = h / float(w) if w else 1.0
                    target_w = max_w_mm * mm
                    target_h = max_h_mm * mm
                    draw_w = target_w
                    draw_h = target_w * aspect
                    if draw_h > target_h:
                        draw_h = target_h
                        draw_w = target_h / aspect

                    # ReportLab ì•ˆì •ì„± ìœ„í•´ ì„ì‹œ PNG ìƒì„±
                    tmp = os.path.join(WORKDIR, f"sec_thumb_p{p}_{time.time()}.png")
                    pil.resize(
                        (int(draw_w/mm * DPI / 25.4), int(draw_h/mm * DPI / 25.4)),
                        PILImage.LANCZOS
                    ).convert("RGB").save(tmp, "PNG")
                    temp_files.append(tmp)

                story.append(Spacer(1, 2*mm))
                story.append(Paragraph(f"ìŠ¬ë¼ì´ë“œ ë¯¸ë‹ˆë·° (p.{p})", styles["SmallGrey"]))
                story.append(FixedImage(tmp, width=draw_w, height=draw_h))
                story.append(Spacer(1, 4*mm))
            except Exception as e:
                log(f"[WARN] ì„¹ì…˜ ì¸ë„¤ì¼ ë Œë” ì‹¤íŒ¨ p.{p}: {e}")
        return temp_files
    
    # [ì¶”ê°€ 5] ì¸ë„¤ì¼ ë Œë”ë§ í•¨ìˆ˜
    def render_highlight_thumbnails(highlight_pages: List[int]):
        THUMBNAIL_WIDTH_MM = 50 * mm # ì¸ë„¤ì¼ ë„ˆë¹„ 50mm
        MAX_THUMB_H = 60 * mm
        
        # í˜ì´ì§€ ë²ˆí˜¸ë³„ ì´ë¯¸ì§€ ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš° ë¦¬í„´
        if not image_paths_map or not image_paths_map.page_to_path:
            return

        story.append(Spacer(1, 4*mm))
        story.append(Paragraph("ğŸ“Œ AIê°€ ì„ ì •í•œ í•µì‹¬ í˜ì´ì§€", styles["HighlightHeading"]))
        
        # ì¸ë„¤ì¼ ë°°ì¹˜ë¥¼ ìœ„í•œ í…Œì´ë¸” ì •ì˜
        MAX_COLS = 3  # í•œ ì¤„ì— ìµœëŒ€ 3ê°œ ë°°ì¹˜
        current_row = []
        col_widths = [doc.width / MAX_COLS] * MAX_COLS
        table_data = []
        
        temp_files = []

        for page_num in highlight_pages:
            img_path = image_paths_map.page_to_path.get(page_num)
            
            if img_path and os.path.exists(img_path):
                temp_png_path = None
                try:
                    # 1. PILë¡œ ì´ë¯¸ì§€ ì—´ê¸° ë° í¬ê¸° ê³„ì‚°
                    # with Image.open(img_path) as img_pil: # ì›ë³¸ ì´ë¯¸ì§€ ì—´ê¸°
                    # Note: JPEG ë©”íƒ€ë°ì´í„° ì˜¤ë¥˜ íšŒí”¼ë¥¼ ìœ„í•´, íŒŒì¼ì„ ì§ì ‘ ì½ì§€ ì•Šê³  
                    # ë Œë”ë§ì— í•„ìš”í•œ PIL ë¦¬ì‚¬ì´ì¦ˆ ë° PNG ë³€í™˜ì„ ìˆ˜í–‰
                    with PILImage.open(img_path) as img_pil:
                        original_width, original_height = img_pil.size
                        aspect_ratio = original_height / original_width
                        thumb_height = min(MAX_THUMB_H, THUMBNAIL_WIDTH_MM * aspect_ratio) 
                        
                        thumb_width_px = int(THUMBNAIL_WIDTH_MM / mm * DPI / 25.4) # mm -> pixel (DPI 150 ê¸°ì¤€)
                        # PIL ë¦¬ì‚¬ì´ì¦ˆ. PIL ê°ì²´ëŠ” ë°˜ë“œì‹œ RGBì—¬ì•¼ ReportLabì—ì„œ ì˜¤ë¥˜ê°€ ì ìŒ.
                        img_resized = img_pil.resize((thumb_width_px, int(thumb_width_px * aspect_ratio)), PILImage.LANCZOS).convert("RGB")
                        # ReportLab Imageì— ì „ë‹¬í•  JPEG ë°ì´í„° ì¤€ë¹„ (ë©”ëª¨ë¦¬ ë²„í¼ ì‚¬ìš©)


                        temp_png_path = os.path.join(WORKDIR, f"temp_thumb_{page_num}_{time.time()}.png")
                        img_resized.save(temp_png_path, format="PNG")
                        temp_files.append(temp_png_path)
                        
                        # kind='absolute'ëŠ” ReportLabì—ê²Œ í¬ê¸° ê³„ì‚°ì„ í•˜ì§€ ë§ê³  ëª…ì‹œëœ width/heightë¥¼ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œ
                        rl_img = FixedImage(temp_png_path, width=THUMBNAIL_WIDTH_MM, height=thumb_height)
                        
                        # ì´ë¯¸ì§€ ì•„ë˜ì— í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                        caption = Paragraph(f"[P. {page_num}]", styles["HighlightPage"])
                        
                        MAX_CELL_H = 70 * mm
                        cell_box = KeepInFrame(col_widths[0], MAX_CELL_H, content=[rl_img, caption], mode="shrink")
                        current_row.append(cell_box)
    
                        # í˜„ì¬ í–‰ì´ ê½‰ ì°¼ìœ¼ë©´ í…Œì´ë¸” ë°ì´í„°ì— ì¶”ê°€í•˜ê³  ìƒˆ í–‰ ì‹œì‘
                        if len(current_row) == MAX_COLS:
                            table_data.append(current_row)
                            current_row = []
                except Exception as e:
                    # íŒŒì¼ ì½ê¸° ë˜ëŠ” ReportLab ê°ì²´ ìƒì„± ì‹œ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê±´ë„ˆë›°ê³  ê²½ê³ 
                    log(f"[WARN] ì¸ë„¤ì¼ ë Œë”ë§ ì‹¤íŒ¨ (P.{page_num}): {e}")
                    pass

        # ë§ˆì§€ë§‰ ë‚¨ì€ í–‰ ì¶”ê°€
        if current_row:
            # ë‚¨ì€ ì…€ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›Œ í…Œì´ë¸” êµ¬ì¡° ìœ ì§€
            while len(current_row) < MAX_COLS:
                current_row.append("")
            table_data.append(current_row)

        if table_data:
            thumb_table = Table(table_data, colWidths=col_widths, hAlign='LEFT')
            # ì¸ë„¤ì¼ í…Œì´ë¸” ìŠ¤íƒ€ì¼
            thumb_table.setStyle(TableStyle([
                ('LEFTPADDING', (0,0), (-1,-1), 2),
                ('RIGHTPADDING', (0,0), (-1,-1), 6), # ì˜¤ë¥¸ìª½ ê°„ê²©
                ('TOPPADDING', (0,0), (-1,-1), 2),
                ('BOTTOMPADDING', (0,0), (-1,-1), 6), # ì•„ë˜ ê°„ê²©
                ('VALIGN', (0,0), (-1,-1), 'TOP'),
                ('ALIGN', (0,0), (-1,-1), 'LEFT'),
            ]))
            story.append(thumb_table)
            story.append(section_rule())
            story.append(Spacer(1, 4*mm))
        return temp_files

    # ì–´ë–¤ í‹°ì–´ë¥¼ PDFë¡œ ì“¸ì§€ ì„ íƒ (ì˜ˆ: standard ìš°ì„ , ì—†ìœ¼ë©´ detailedâ†’short)
    summaries = data.get("summaries", {})
    tier = None
    
    for cand in ("detailed", "standard", "short"):
        t = summaries.get(cand)
        if _nonempty_tier(t):
            tier = t
            break
    if tier is None:
        tier = {"sections": []}
    
    highlight_pool = list(data.get("highlight_pages", []))
        
    for sec in tier.get("sections", []):
        h = sec.get("h2", "").strip() or "Untitled"
        paragraphs = sec.get("paragraphs", [])
        bullets = sec.get("bullets", [])
        tables = sec.get("tables", [])

        block = []
        block.append(Paragraph(h, styles["H2KR"]))
        if rule_on:
            block.append(section_rule())

        for p in paragraphs:
            block.append(Paragraph(p, styles["BodyKR"]))
            block.append(Spacer(1, 1*mm))

        for b in bullets_flow(bullets):
            block.append(b)

        for tspec in tables:
            block.extend(table_from_spec(tspec))
            block.append(Spacer(1, 2*mm))

        story.append(KeepTogether(block))
        story.append(Spacer(1, 4*mm))
        section_pages = _pages_for_section(sec, highlight_pool)
        temp_files_to_clean.extend(
            _append_section_thumbnail(story, section_pages, image_paths_map, styles)
        )

    # Glossary ë Œë”ë§ (Blank ëª¨ë“œ ë°ì´í„° ë³´ì¡´)
    glossary = data.get("glossary", [])
    if glossary and MODE == "summary":
        story.append(PageBreak())
        story.append(Paragraph("ìš©ì–´ ì •ë¦¬ (Glossary)", styles["H2KR"]))
        story.append(section_rule())
        
        # ìš©ì–´ì§‘ì€ 2ì—´ í…Œì´ë¸”ë¡œ ë Œë”ë§
        glossary_table = Table([[Paragraph(c, styles["BodyKR"]) for c in row] for row in glossary], colWidths=[doc.width * 0.3, doc.width * 0.7], hAlign='LEFT')
        glossary_table.setStyle(TableStyle([
            ('FONTNAME', (0,0), (-1,-1), 'HYSMyeongJo-Medium'),
            ('VALIGN', (0,0), (-1,-1), 'TOP'),
            ('GRID', (0,0), (-1,-1), 0.25, colors.HexColor("#DDDDDD")),
            ('LEFTPADDING', (0,0), (-1,-1), 4),
            ('RIGHTPADDING', (0,0), (-1,-1), 4),
        ]))
        story.append(glossary_table)
        story.append(Spacer(1, 4*mm))

    checklist = data.get("checklist", [])
    if checklist and MODE == "summary":
        story.append(PageBreak())
        story.append(Paragraph("í™•ì¸ ëª©ë¡ (Checklist)", styles["H2KR"]))
        story.append(section_rule())
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ëŠ” ë²ˆí˜¸ ì—†ëŠ” ëª©ë¡ìœ¼ë¡œ ë Œë”ë§
        for item in checklist:
            # â¬œ ìœ ë‹ˆì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´í¬ë°•ìŠ¤ì²˜ëŸ¼ ë³´ì´ê²Œ í•¨
            story.append(Paragraph(f"â¬œ {item}", styles["BodyKR"]))
            story.append(Spacer(1, 1*mm))
        story.append(Spacer(1, 4*mm))

    answer_sheet = data.get("answer_sheet", None)
    if answer_sheet and isinstance(answer_sheet, dict):
        items = answer_sheet.get("items", [])
        if items:
            ans_title = answer_sheet.get("title", "ì •ë‹µ ëª¨ìŒ")
            story.append(PageBreak())
            story.append(Paragraph(ans_title, styles["H2KR"]))
            if rule_on:
                story.append(section_rule())
            for it in items:
                story.append(Paragraph(it, styles["BodyKR"]))

    doc.build(story)
    return temp_files_to_clean

# ======================== ë©”ì¸ ========================
def main():
    if not os.path.exists(PDF_FILE):
        raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_FILE}")

    current_pdf_hash = sha256_file(PDF_FILE)
    last_hash = read_text(INPUT_HASH_PATH).strip()

    if last_hash and last_hash != current_pdf_hash:
        log("â„¹ ì…ë ¥ PDFê°€ ë³€ê²½ë¨ì„ ê°ì§€ â†’ ì²´í¬í¬ì¸íŠ¸/í˜ì´ì§€ ìºì‹œ ì´ˆê¸°í™”")
        # ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
        try:
            if os.path.exists(CHECKPOINT_PATH):
                os.remove(CHECKPOINT_PATH)
        except Exception as e:
            log(f"[WARN] ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}")

        # pages í´ë” ë¹„ìš°ê¸°
        try:
            import glob
            for f in glob.glob(os.path.join(WORKDIR, "pages", "page_*.jpg")):
                os.remove(f)
        except Exception as e:
            log(f"[WARN] pages ì •ë¦¬ ì‹¤íŒ¨: {e}")

    # í˜„ì¬ í•´ì‹œ ì €ì¥(ë‹¤ìŒ ì‹¤í–‰ ëŒ€ë¹„)
    write_text(INPUT_HASH_PATH, current_pdf_hash)

    # 1) PDF â†’ ì´ë¯¸ì§€
    img_dir = os.path.join(WORKDIR, "pages")
    if ALWAYS_CLEAN_PAGES:
        try:
            import glob
            for f in glob.glob(os.path.join(img_dir, "page_*.jpg")):
                os.remove(f)
        except Exception as e:
            log(f"[WARN] ALWAYS_CLEAN_PAGES ì •ë¦¬ ì‹¤íŒ¨: {e}")
    image_paths = pdf_to_images(PDF_FILE, img_dir, dpi=DPI, poppler_path=POPPLER_PATH)
    
    # ì´ë¯¸ì§€ ê²½ë¡œ ë§µ ìƒì„± (ì¸ë„¤ì¼ ì‚½ì…ì„ ìœ„í•¨)
    page_to_path_map = {i + 1: path for i, path in enumerate(image_paths)}
    img_path_manager = ImagePaths(page_to_path_map)

    # 2) í˜ì´ì§€ ë‹¨ìœ„ ìš”ì•½(ì²´í¬í¬ì¸íŠ¸ ì§€ì›)
    page_summaries = summarize_pages(image_paths)  # { "p1":"...", ... }
    total = len(image_paths)

    # 3) í†µí•©ìš”ì²­
    #   í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ í•©ì¹˜ê¸°
    ordered_md = []
    for i in range(1, total+1):
        k = f"p{i}"
        if k in page_summaries:
            ordered_md.append(sanitize_page_md(page_summaries[k]))
    all_pages_md = "\n\n".join(ordered_md)
    
    try:
        save_page_summaries_json(
            page_summaries=page_summaries,
            image_paths=image_paths,
            checkpoint_path=CHECKPOINT_PATH,
            out_path=PAGE_SUMMARIES_JSON_PATH
        )
    except Exception as e:
        log(f"[WARN] í˜ì´ì§€ë³„ ìš”ì•½ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ğŸš¨ [ì¶”ê°€] í†µí•© í”„ë¡¬í”„íŠ¸ í¬ê¸° ê²½ê³ 
    try:
        prompt_bytes = all_pages_md.encode('utf-8')
        if len(prompt_bytes) > MAX_AGGREGATE_PROMPT_BYTES:
            log(f"âš ï¸ [WARNING] í†µí•© í”„ë¡¬í”„íŠ¸ í¬ê¸°ê°€ {len(prompt_bytes)/1024:.0f}KBë¡œ ê³¼ë„í•˜ê²Œ í½ë‹ˆë‹¤. (ì œí•œ: {MAX_AGGREGATE_PROMPT_BYTES/1024:.0f}KB)")
            log("ì´ëŠ” LLM API í˜¸ì¶œ ì‹¤íŒ¨, ì†ë„ ì €í•˜, ë˜ëŠ” ë¹„ìš© ì¦ê°€ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…ë ¥ PDFë¥¼ ë¶„í• í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
    except Exception:
        pass # ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

    if MODE == "summary":
        log("â–¶ í†µí•© ìš”ì•½(JSON) ìƒì„±")
        data = call_llm_on_text(prompt_aggregate_summary(all_pages_md), system_prompt=SYSTEM_PROMPT_VISUAL)
        raw_output_path = SUMMARY_JSON_PATH.replace(".json", "_raw_llm_output.txt")
        write_text(raw_output_path, json.dumps(data, ensure_ascii=False, indent=2))
        log(f"â„¹ LLM ì›ì‹œ ì¶œë ¥ ì €ì¥ (ë””ë²„ê¹…ìš©): {raw_output_path}")

        if not isinstance(data, dict):
            log("[ERROR] LLM ê²°ê³¼ê°€ dictê°€ ì•„ë‹™ë‹ˆë‹¤. _fill_summary_defaults í˜¸ì¶œ ë¶ˆê°€.")
            data = {"raw_output_invalid_type": str(data)}

        # ìŠ¤í‚¤ë§ˆ ê¸°ë³¸ê°’ ì±„ìš°ê¸°(ê²¬ê³ ì„±)
        data = _fill_summary_defaults(data, lang=LANG)
        
        summaries = data.get("summaries", {})
        tier_names = ["standard", "detailed", "short"]
        selected_tier = None
        for cand in tier_names:
            t = summaries.get(cand)
            if _nonempty_tier(t):
                selected_tier = cand
                break
        
        if selected_tier:
             log(f"â„¹ PDF ë Œë”ë§ì— ì‚¬ìš©ë  í‹°ì–´: {selected_tier}. ì„¹ì…˜ ìˆ˜: {len(data['summaries'][selected_tier]['sections'])}")
        else:
             log("[WARN] PDF ë Œë”ë§ì— ì‚¬ìš©í•  ìœ íš¨í•œ ìš”ì•½ ì„¹ì…˜(standard/detailed/short)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        try:
            hps = data.get("highlight_pages", [])
            if isinstance(hps, list):
                with open(HIGHLIGHT_JSON_PATH, "w", encoding="utf-8") as f:
                    json.dump({"highlight_pages": hps}, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log(f"[WARN] highlight_pages ì €ì¥ ì‹¤íŒ¨: {e}")

        with open(SUMMARY_JSON_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        log(f"âœ… ìš”ì•½ JSON ì €ì¥: {SUMMARY_JSON_PATH}")
        
        temp_files_to_clean = [] # ì •ë¦¬í•  ì„ì‹œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        try:
            # 2) âœ… JSON â†’ PDF ë³€í™˜ í˜¸ì¶œ ë° ì„ì‹œ íŒŒì¼ ëª©ë¡ ë°›ê¸°
            temp_files_to_clean = export_pdf_from_json(data, SUMMARY_PDF_PATH, image_paths_map=img_path_manager) # ğŸš¨ ë°˜í™˜ ê°’ ë°›ê¸°
            log(f"âœ… ìš”ì•½ PDF ì €ì¥: {SUMMARY_PDF_PATH}")
        finally:
            # ğŸš¨ [í•µì‹¬ ìˆ˜ì •]: PDF ìƒì„±ì´ ì™„ë£Œëœ í›„ ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for f in temp_files_to_clean:
                try:
                    os.remove(f)
                    log(f"â„¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬: {os.path.basename(f)}")
                except Exception as e:
                    log(f"[WARN] ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {os.path.basename(f)}, {e}")
            if temp_files_to_clean:
                log(f"â„¹ ì´ {len(temp_files_to_clean)}ê°œ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ.")

    elif MODE == "blank":
        log("â–¶ ë¹ˆì¹¸ ì±„ìš°ê¸° ë…¸íŠ¸(JSON) ìƒì„±")
        min_clozes = round(total / 3)
        data = call_llm_on_text(prompt_aggregate_blank(all_pages_md, min_clozes), system_prompt=SYSTEM_PROMPT_VISUAL)
        raw_output_path = BLANK_JSON_PATH.replace(".json", "_raw_llm_output.txt") # ğŸš¨ [ì¶”ê°€] ì›ì‹œ ì¶œë ¥ ì €ì¥
        write_text(raw_output_path, json.dumps(data, ensure_ascii=False, indent=2))
        log(f"â„¹ LLM ì›ì‹œ ì¶œë ¥ ì €ì¥ (ë””ë²„ê¹…ìš©): {raw_output_path}")

        
        if not data:
            log("[ERROR] JSON íŒŒì‹±/ë³µêµ¬ ìµœì¢… ì‹¤íŒ¨. ì›ì‹œ ì¶œë ¥ í™•ì¸ í•„ìš”.")
            data = {"raw_output_could_not_be_parsed": data}

        data = _fill_summary_defaults(data, lang=LANG) 

        print(data)
        clozes = data.get("clozes", [])
        if not clozes:
             log("âš ï¸ [WARNING] LLMì´ 'clozes' ëª©ë¡ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFì— ë¹ˆì¹¸ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.")
             log(f"ğŸ’¡ LLM ì¶œë ¥ íŒŒì¼({os.path.basename(raw_output_path)})ì„ í™•ì¸í•˜ì—¬ clozes í•„ë“œ ëˆ„ë½ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.")
        # ë³¸ë¬¸: ì •ë‹µ ìˆ¨ê¹€
        bullets = [c.get("text", "") for c in clozes]
        # ë§ˆì§€ë§‰ ì¥ ì •ë‹µ ëª¨ìŒ
        answers = [f"{i+1}. {c.get('answer','')}" for i, c in enumerate(clozes)]
        
        standard_summaries = data.get("summaries", {}).get("standard", {})
        glossary = []
        checklist = []

        mapped = {
            "meta": {"title":"ë¹ˆì¹¸ ì±„ìš°ê¸° ë…¸íŠ¸ (ì •ë‹µ ìˆ¨ê¹€)","course":"","date":time.strftime("%Y-%m-%d"),"language":LANG},
            "style": {"section_rule": True, "tables":{"header_color":"#EAF3FF","zebra_stripe":True}},
            "summaries": {
                "standard": {
                    "sections": [{
                        "h2": "Fill-in-the-Blank",
                        "paragraphs": [
                            "ë¹ˆì¹¸(____)ì— ë“¤ì–´ê°ˆ í•µì‹¬ ìš©ì–´/ìˆ«ì/ê¸°í˜¸ë¥¼ ì±„ìš°ì„¸ìš”.",
                            "ì •ë‹µì€ ë¬¸ì„œ ë§¨ ë§ˆì§€ë§‰ 'ì •ë‹µ ëª¨ìŒ' í˜ì´ì§€ì— ìˆìŠµë‹ˆë‹¤."
                        ],
                        "bullets": bullets
                    }],

                }
            },
            "highlight_pages": [],
            # âœ… ì •ë‹µ ëª¨ìŒì€ ë Œë”ëŸ¬ì—ì„œ ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ ì¶œë ¥
            "answer_sheet": {
                "title": "ì •ë‹µ ëª¨ìŒ",
                "items": answers
            }
        }
        
        temp_files_to_clean = [] # ì •ë¦¬í•  ì„ì‹œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        try:
            temp_files_to_clean = export_pdf_from_json(mapped, BLANK_PDF_PATH, image_paths_map=img_path_manager) 
            log(f"âœ… ë¹ˆì¹¸ PDF ì €ì¥(ì •ë‹µ ìˆ¨ê¹€ + ë§ˆì§€ë§‰ ì¥ ì •ë‹µ ëª¨ìŒ): {BLANK_PDF_PATH}")
        finally:
            for f in temp_files_to_clean:
                try:
                    os.remove(f)
                    log(f"â„¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬: {os.path.basename(f)}")
                except Exception as e:
                    log(f"[WARN] ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {os.path.basename(f)}, {e}")
            if temp_files_to_clean:
                log(f"â„¹ ì´ {len(temp_files_to_clean)}ê°œ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ.")
        
    elif MODE == "quiz":
        log("â–¶ ì˜ˆìƒ ë¬¸ì œ(JSON) ìƒì„±")
        min_clozes = round(total / 3)
        allow_short_answer = QUIZ_ALLOW_SHORT_ANSWER
        data = call_llm_on_text(
            prompt_aggregate_quiz(all_pages_md, min_clozes, allow_short_answer),
            system_prompt=SYSTEM_PROMPT_VISUAL,
        )
        raw_output_path = QUIZ_JSON_PATH.replace(".json", "_raw_llm_output.txt")
        write_text(raw_output_path, json.dumps(data, ensure_ascii=False, indent=2))
        log(f"â„¹ LLM ì›ì‹œ ì¶œë ¥ ì €ì¥: {raw_output_path}")

        quiz_json = {}

        if not isinstance(data, dict):
            quiz_json = {"raw": str(data)}
        elif "multiple_choice" in data or "short_answer" in data:
            quiz_json = data
        elif "clozes" in data:
            quiz_json = {
                "multiple_choice": [],
                "short_answer": [
                    {"q": c.get("text", ""), "a": c.get("answer", "")}
                    for c in data.get("clozes", [])
                ]
            }
        else:
            quiz_json = {"raw": data}
            
        with open(QUIZ_JSON_PATH, "w", encoding="utf-8") as f:
            json.dump(quiz_json, f, ensure_ascii=False, indent=2)
        log(f"âœ… ì˜ˆìƒ ë¬¸ì œ JSON ì €ì¥: {QUIZ_JSON_PATH}")

    else:
        raise ValueError("MODE must be one of: summary | blank | quiz")

if __name__ == "__main__":
    main()